---
title: Statistical Learning
published: 2024-09-17
description: Some basic concepts of statistical learning
tags: [StatisticalLearning]
category: MachineLearning
draft: false
---


## 回归

统计学家发现人类身高总是回归到平均值，这就是回归的由来。

## 拉格朗日乘子

拉格朗日乘子主要是利用了L和constraint的之间在空间中相切的性质，从而将原问题转化为一个无约束的问题。

求导为0既有可能是极大值，也有可能是极小值，所以需要验证。多个限制的时候，一定要把所有的限制条件都考虑进去。

可以参考文件：[拉格朗日乘子](https://zh.wikipedia.org/wiki/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90)

题目：
Use the Lagrange multiplier method to solve the following optimization problem:

$ \min _{x_1, x_2} 10-x_1^2-x_2^2, \text { subject to } x_2 \geq x_1^2, x_1+x_2=0 $

对于$ \geq $等带等好的约束条件，可以先忽略，然后再验证，如果不满足，则转化为等式。

## KKT
KKT条件，全称Karush-Kuhn-Tucker条件，是一组非线性规划问题中必须满足的数学条件。
具体而言，KKT条件适用于带有约束的最优化问题，特别是那些约束条件包括等式和不等式的问题。在满足一定的正则条件（如约束的梯度线性无关或Slater条件）下，如果一个点是局部最优解，那么必定满足KKT条件。

KKT条件包括以下几个部分：
1. **稳定性条件**：目标函数和约束条件的梯度线性组合必须为零。
2. **原始可行性**：解必须满足问题中所有的约束条件。
3. **对偶可行性**：引入的拉格朗日乘数必须非负，这适用于不等式约束。
4. **互补松弛性**：拉格朗日乘数和对应约束的乘积必须为零，意味着若约束未严格满足（即在不等式约束的边界上），则对应的乘数为零。

这些条件是理解和解决优化问题中非常关键的一部分，尤其是在寻找存在复杂约束的问题的解时。

对于包含不等式约束（大于等于或小于等于）的优化问题，KKT条件的应用也类似，但增加了对偶可行性和互补松弛性条件的考虑。我们来看一个具有不等式约束的例子来更好地理解这些条件的应用。

#### 问题设定

最小化目标函数：
$$ f(x, y) = x^2 + y^2 $$

约束条件：
$$ g(x, y) = x + y - 1 \geq 0 $$

在这个问题中，我们需要找到 \( (x, y) \) 的值，使 \( x^2 + y^2 \) 最小，同时满足 \( x + y \geq 1 \)。

#### 构建拉格朗日函数

拉格朗日函数 \(L\) 定义为：
$$ L(x, y, \lambda) = x^2 + y^2 + \lambda (x + y - 1) $$

#### 计算偏导数

求解 \(L\) 关于 \(x, y, \lambda\) 的偏导数，并设置它们等于零（除了对 \(\lambda\) 的偏导，因为它对应于不等式约束）：
$$ \frac{\partial L}{\partial x} = 2x + \lambda = 0 $$
$$ \frac{\partial L}{\partial y} = 2y + \lambda = 0 $$
$$ \frac{\partial L}{\partial \lambda} = x + y - 1 $$

#### KKT条件

- **稳定性条件**：
    - $$ 2x + \lambda = 0 $$
    - $$ 2y + \lambda = 0 $$

- **原始可行性**：
    - $$ x + y \geq 1 $$

- **对偶可行性**：
    - $$ \lambda \geq 0 $$（因为约束是大于等于类型）

- **互补松弛性**：
    - $$ \lambda (x + y - 1) = 0 $$

#### 解析方程和条件

根据互补松弛性，我们有两种情况：

**情况1: \(\lambda > 0\)**
- 这意味着 \( x + y - 1 = 0 \)（约束严格满足）

**情况2: \(\lambda = 0\)**
- 这意味着约束可能是非活跃的或者正好在边界上
